# KUBERNETES CLUSTER WITH RANCHER - COMPLETE SETUP GUIDE
# Author: Step-by-Step Setup for Ubuntu 22.04
# Date: November 4, 2025
# Infrastructure: 3 Servers with Rancher Management

===========================================
INFRASTRUCTURE OVERVIEW
===========================================

SERVER DETAILS:
- Master Node: 10.10.80.76 (k8s-master) - Ubuntu 22.04
- Worker Node 1: 10.10.80.77 (k8s-node1) - Ubuntu 22.04  
- Worker Node 2: 10.10.80.78 (k8s-node2) - Ubuntu 22.04

COMPONENTS TO INSTALL:
- Kubernetes v1.28
- Docker Container Runtime
- Calico CNI Network Plugin
- Helm Package Manager
- Cert-Manager
- Rancher Server
- NGINX Ingress Controller

===========================================
SECTION 1: PREREQUISITES AND PREPARATION
===========================================

1.1 UPDATE ALL SERVERS
Run on ALL three servers (76, 77, 78):

sudo apt update && sudo apt upgrade -y
sudo apt install -y curl wget apt-transport-https ca-certificates gnupg lsb-release

1.2 DISABLE SWAP ON ALL SERVERS
Run on ALL three servers:

sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

1.3 CONFIGURE HOSTNAMES
Run on Master (10.10.80.76):
sudo hostnamectl set-hostname k8s-master

Run on Node 1 (10.10.80.77):
sudo hostnamectl set-hostname k8s-node1

Run on Node 2 (10.10.80.78):
sudo hostnamectl set-hostname k8s-node2

1.4 UPDATE /etc/hosts FILE ON ALL SERVERS
Add these lines to /etc/hosts on ALL servers:

sudo tee -a /etc/hosts <<EOF
10.10.80.76 k8s-master
10.10.80.77 k8s-node1  
10.10.80.78 k8s-node2
10.10.80.76 rancher.local.cluster
EOF

1.5 CONFIGURE FIREWALL (if enabled)
Run on ALL servers:

# For master node (run only on 10.10.80.76):
sudo ufw allow 6443/tcp
sudo ufw allow 2379:2380/tcp
sudo ufw allow 10250/tcp
sudo ufw allow 10259/tcp
sudo ufw allow 10257/tcp

# For worker nodes (run on 10.10.80.77 and 10.10.80.78):
sudo ufw allow 10250/tcp
sudo ufw allow 30000:32767/tcp

# For Rancher and Ingress (run on all nodes):
sudo ufw allow 80/tcp
sudo ufw allow 443/tcp
sudo ufw allow 31680/tcp
sudo ufw allow 32085/tcp

===========================================
SECTION 2: INSTALL CONTAINER RUNTIME (DOCKER)
===========================================

2.1 INSTALL DOCKER ON ALL SERVERS
Run on ALL three servers:

# Add Docker's official GPG key:
sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg

# Add Docker repository:
echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

# Install Docker:
sudo apt update
sudo apt install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

# Start and enable Docker:
sudo systemctl start docker
sudo systemctl enable docker

# Add your user to docker group:
sudo usermod -aG docker $USER

2.2 CONFIGURE CONTAINERD
Run on ALL servers:

sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml

# Enable SystemdCgroup in containerd:
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml

# Restart containerd:
sudo systemctl restart containerd
sudo systemctl enable containerd

===========================================
SECTION 3: INSTALL KUBERNETES COMPONENTS
===========================================

3.1 ADD KUBERNETES REPOSITORY
Run on ALL servers:

# Add Kubernetes signing key:
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

# Add Kubernetes repository:
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

3.2 INSTALL KUBERNETES PACKAGES
Run on ALL servers:

sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

# Enable kubelet:
sudo systemctl enable kubelet

3.3 CONFIGURE KERNEL MODULES
Run on ALL servers:

# Load required modules:
sudo modprobe overlay
sudo modprobe br_netfilter

# Make modules persistent:
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

# Configure sysctl parameters:
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

# Apply sysctl parameters:
sudo sysctl --system

===========================================
SECTION 4: INITIALIZE KUBERNETES CLUSTER
===========================================

4.1 INITIALIZE MASTER NODE
Run ONLY on Master (10.10.80.76):

sudo kubeadm init \
  --apiserver-advertise-address=10.10.80.76 \
  --pod-network-cidr=10.244.0.0/16 \
  --control-plane-endpoint=10.10.80.76

# IMPORTANT: Save the join command from the output!
# Example join command:
# kubeadm join 10.10.80.76:6443 --token l8anab.whwzakko6mt5k463 \
#     --discovery-token-ca-cert-hash sha256:737df87f1c4f6aa9ed5abae8c6436c2a1e221bb0bad70c94456738fcdb448fdc

4.2 CONFIGURE KUBECTL FOR REGULAR USER
Run ONLY on Master (10.10.80.76):

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

4.3 VERIFY MASTER NODE
Run on Master:

kubectl get nodes
kubectl get pods -A

===========================================
SECTION 5: INSTALL CNI NETWORK PLUGIN
===========================================

5.1 INSTALL CALICO
Run ONLY on Master (10.10.80.76):

# Install Calico CNI:
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/tigera-operator.yaml

# Download and apply custom resources:
curl https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/custom-resources.yaml -O

# Edit the custom-resources.yaml to match your pod CIDR (10.244.0.0/16):
sed -i 's|192.168.0.0/16|10.244.0.0/16|g' custom-resources.yaml

# Apply the custom resources:
kubectl create -f custom-resources.yaml

# Wait for Calico pods to be ready:
kubectl get pods -n calico-system

# Wait for master node to become Ready:
kubectl get nodes

# Verify Calico installation:
kubectl get pods -n calico-system -w

===========================================
SECTION 6: INSTALL MONITORING STACK (PROMETHEUS & GRAFANA)
===========================================

6.1 PRODUCTION MONITORING SETUP (WORKER NODES ONLY)
âš ï¸ PRODUCTION BEST PRACTICE: Deploy monitoring on WORKER nodes ONLY, not master!

Run on Master (10.10.80.76) - Commands only:

# Step 1: Label and configure nodes for production
kubectl label node k8s-node1 node-role.kubernetes.io/worker=true --overwrite
kubectl label node k8s-node2 node-role.kubernetes.io/worker=true --overwrite
kubectl label node k8s-node1 monitoring=enabled --overwrite
kubectl label node k8s-node2 monitoring=enabled --overwrite

# Taint master node to prevent workload scheduling (production best practice)
kubectl taint nodes k8s-master node-role.kubernetes.io/master=true:NoSchedule --overwrite
kubectl taint nodes k8s-master node-role.kubernetes.io/control-plane=true:NoSchedule --overwrite

# Step 2: AUTOMATED PRODUCTION INSTALLATION (RECOMMENDED)
cd /home/kp/oraganization-project/k8-cluster/monitoring
./install-production-monitoring.sh

# OR MANUAL INSTALLATION:
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
kubectl create namespace monitoring
helm install prometheus-stack prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --values /home/kp/oraganization-project/k8-cluster/monitoring/production-values.yaml \
  --timeout 20m \
  --wait

# Wait for deployment to complete:
kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=grafana -n monitoring --timeout=600s
kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=prometheus -n monitoring --timeout=600s

6.2 VERIFY PRODUCTION MONITORING DEPLOYMENT
# Check monitoring pods (should be on WORKER NODES ONLY):
kubectl get pods -n monitoring -o wide

# Verify NO monitoring pods on master node:
kubectl get pods -n monitoring -o wide | grep k8s-master
# This should return NO results for production deployment!

# Check monitoring services:
kubectl get svc -n monitoring

# Verify persistent storage:
kubectl get pvc -n monitoring

# PRODUCTION ACCESS URLs:
echo "Grafana: https://grafana.k8s.local (Production)"
echo "Prometheus: https://prometheus.k8s.local (Production)"
echo "AlertManager: https://alertmanager.k8s.local (Production)"
echo ""
echo "Development Access:"
echo "Grafana: http://10.10.80.77:31000 or http://10.10.80.78:31000"
echo "Prometheus: http://10.10.80.77:31001 or http://10.10.80.78:31001"
echo "AlertManager: http://10.10.80.77:31002 or http://10.10.80.78:31002"

6.3 PRODUCTION MONITORING ACCESS
ðŸ­ PRODUCTION ARCHITECTURE:
âœ… Master Node (k8s-master): Control plane + lightweight monitoring
   - Kubernetes API Server, etcd, Scheduler (fully monitored)
   - Node Exporter for system metrics
   - NO heavy workloads (Grafana/Prometheus servers)
âœ… Worker Node 1 (k8s-node1): Full monitoring stack + Applications
âœ… Worker Node 2 (k8s-node2): Full monitoring stack + Applications

PRODUCTION ACCESS (HTTPS with Ingress):
- Grafana: https://grafana.k8s.local
  Username: admin | Password: ProD@Grafan@2025! (CHANGE IN PRODUCTION!)
  Features: HA dashboards, persistent storage, production-grade security

- Prometheus: https://prometheus.k8s.local
  Features: HA metrics collection, 30-day retention, worker node deployment

- AlertManager: https://alertmanager.k8s.local
  Features: HA alert routing, persistent configuration, production notifications

DEVELOPMENT ACCESS (HTTP NodePort):
- Grafana: http://10.10.80.77:31000 or http://10.10.80.78:31000
- Prometheus: http://10.10.80.77:31001 or http://10.10.80.78:31001
- AlertManager: http://10.10.80.77:31002 or http://10.10.80.78:31002

6.4 PRODUCTION MONITORING FILES
All production monitoring configuration files:
/home/kp/oraganization-project/k8-cluster/monitoring/

PRODUCTION FILES:
- production-monitoring-setup.txt (Complete production guide)
- install-production-monitoring.sh (Production installation script) â­ USE THIS
- production-values.yaml (Production Helm values with HA/security)
- production-ingress.yaml (Production ingress with TLS/auth)

DEVELOPMENT FILES:
- prometheus-grafana-setup.txt (Basic setup guide)
- install-monitoring.sh (Basic installation script)
- monitoring-values.yaml (Basic Helm values)
- monitoring-ingress.yaml (Basic ingress configuration)

PRODUCTION FEATURES:
âœ“ High availability (2+ replicas)
âœ“ Worker node deployment only
âœ“ Persistent storage with backup
âœ“ Resource limits and security
âœ“ TLS/SSL ready configuration
âœ“ Network policies and RBAC
âœ“ Production-grade access controls

===========================================
SECTION 7: JOIN WORKER NODES
===========================================

6.1 JOIN WORKER NODES TO CLUSTER
Run on Worker Node 1 (10.10.80.77) and Worker Node 2 (10.10.80.78):

# Use your specific join command from step 4.1:
sudo kubeadm join 10.10.80.76:6443 --token l8anab.whwzakko6mt5k463 \
    --discovery-token-ca-cert-hash sha256:737df87f1c4f6aa9ed5abae8c6436c2a1e221bb0bad70c94456738fcdb448fdc

# If you lost the join command, generate a new one on master:
# kubeadm token create --print-join-command

6.2 VERIFY CLUSTER
Run on Master (10.10.80.76):

kubectl get nodes
# All nodes should show "Ready" status

kubectl get pods -A
# All system pods should be "Running"

===========================================
SECTION 8: INSTALL HELM (for Rancher)
===========================================

8.1 INSTALL HELM ON MASTER
Run ONLY on Master (10.10.80.76):

# Method 1: Direct installation (Recommended)
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh
rm get_helm.sh

# Alternative Method 2: Manual download
# wget https://get.helm.sh/helm-v3.13.2-linux-amd64.tar.gz
# tar -zxvf helm-v3.13.2-linux-amd64.tar.gz
# sudo mv linux-amd64/helm /usr/local/bin/helm
# sudo chmod +x /usr/local/bin/helm
# rm -rf helm-v3.13.2-linux-amd64.tar.gz linux-amd64/

# Verify installation:
helm version

===========================================
SECTION 9: INSTALL CERT-MANAGER
===========================================

9.1 INSTALL CERT-MANAGER
Run on Master (10.10.80.76):

# Add cert-manager repository:
helm repo add jetstack https://charts.jetstack.io
helm repo update

# Install cert-manager:
kubectl create namespace cert-manager
helm install cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --version v1.13.0 \
  --set installCRDs=true

# Verify installation:
kubectl get pods -n cert-manager

# Wait for all cert-manager pods to be Running:
kubectl wait --for=condition=ready pod -l app.kubernetes.io/instance=cert-manager -n cert-manager --timeout=300s

===========================================
SECTION 10: INSTALL NGINX INGRESS CONTROLLER
===========================================

10.1 INSTALL NGINX INGRESS CONTROLLER
Run on Master (10.10.80.76):

# Install NGINX Ingress Controller:
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.2/deploy/static/provider/baremetal/deploy.yaml

# Wait for deployment to complete:
kubectl wait --namespace ingress-nginx \
  --for=condition=ready pod \
  --selector=app.kubernetes.io/component=controller \
  --timeout=300s

# Verify installation:
kubectl get pods -n ingress-nginx
kubectl get svc -n ingress-nginx

# Note the NodePorts (typically 31680 for HTTP, 32085 for HTTPS):
# NAME                                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
# ingress-nginx-controller             NodePort    10.104.135.106   <none>        80:31680/TCP,443:32085/TCP   62s

===========================================
SECTION 11: INSTALL RANCHER
===========================================

11.1 ADD RANCHER HELM REPOSITORY
Run on Master (10.10.80.76):

helm repo add rancher-latest https://releases.rancher.com/server-charts/latest
helm repo update

11.2 CREATE RANCHER NAMESPACE
kubectl create namespace cattle-system

11.3 INSTALL RANCHER
Run on Master (10.10.80.76):

# Install Rancher with proper hostname:
helm install rancher rancher-latest/rancher \
  --namespace cattle-system \
  --set hostname=rancher.local.cluster \
  --set bootstrapPassword=admin123456 \
  --set ingress.tls.source=rancher \
  --set replicas=1

# Wait for Rancher deployment:
kubectl -n cattle-system rollout status deploy/rancher --timeout=600s

# Verify Rancher pods:
kubectl get pods -n cattle-system

11.4 FIX INGRESS CLASS (Important!)
kubectl patch ingress rancher -n cattle-system -p '{"spec":{"ingressClassName":"nginx"}}'

===========================================
SECTION 12: CREATE DIRECT ACCESS TO RANCHER
===========================================

12.1 CREATE NODEPORT FOR DIRECT ACCESS
Run on Master (10.10.80.76):

# Create NodePort service for direct access:
kubectl expose deployment rancher -n cattle-system \
  --type=NodePort \
  --name=rancher-nodeport \
  --port=443 \
  --target-port=8443

# Get the NodePort:
kubectl get svc rancher-nodeport -n cattle-system

# Get access URL:
NODEPORT=$(kubectl get svc rancher-nodeport -n cattle-system -o jsonpath='{.spec.ports[0].nodePort}')
echo "Direct Rancher Access: https://10.10.80.76:$NODEPORT"

===========================================
SECTION 12: ACCESS RANCHER UI
===========================================

12.1 ACCESS METHODS

METHOD 1: Direct NodePort Access (RECOMMENDED)
1. Get NodePort: kubectl get svc rancher-nodeport -n cattle-system
2. Access URL: https://10.10.80.76:NODEPORT
3. Username: admin
4. Password: admin123456

METHOD 2: Via Ingress with Hostname
1. Access URL: https://rancher.local.cluster:32085
2. Username: admin
3. Password: admin123456

METHOD 3: Via Ingress with IP
1. Access URL: https://10.10.80.76:32085
2. Username: admin
3. Password: admin123456

12.2 FIRST TIME LOGIN
- Accept the self-signed certificate warning
- Login with credentials above
- Complete the initial setup wizard
- Set up additional users if needed

===========================================
SECTION 13: VERIFICATION AND TESTING
===========================================

13.1 CLUSTER HEALTH CHECK
Run on Master:

# Check all nodes:
kubectl get nodes -o wide

# Check all pods:
kubectl get pods -A

# Check cluster info:
kubectl cluster-info

# Check services:
kubectl get svc -A

13.2 RANCHER VERIFICATION
# Check Rancher status:
kubectl get pods -n cattle-system
kubectl get svc -n cattle-system
kubectl get ingress -n cattle-system

# Check Ingress Controller:
kubectl get pods -n ingress-nginx
kubectl get svc -n ingress-nginx

13.3 TEST DEPLOYMENT
Create a test deployment:

kubectl create deployment nginx-test --image=nginx:latest --replicas=3
kubectl expose deployment nginx-test --port=80 --target-port=80 --type=NodePort
kubectl get pods -o wide
kubectl get svc nginx-test

# Test access on any node IP with the assigned NodePort
# curl http://10.10.80.76:NODEPORT

===========================================
SECTION 14: TROUBLESHOOTING GUIDE
===========================================

14.1 COMMON ISSUES AND SOLUTIONS

ISSUE: Nodes not joining cluster
SOLUTION: 
- Check firewall rules
- Verify network connectivity
- Ensure ports 6443, 10250 are open
- Generate new join token: kubeadm token create --print-join-command

ISSUE: Pods stuck in Pending state
SOLUTION: 
- Check CNI plugin: kubectl get pods -n calico-system
- Check node resources: kubectl describe nodes
- Check for taints: kubectl describe node k8s-master

ISSUE: Rancher 404 error
SOLUTION:
- Use NodePort access method
- Verify Ingress class: kubectl get ingress rancher -n cattle-system
- Check Rancher pods: kubectl get pods -n cattle-system

ISSUE: Certificate errors in browser
SOLUTION:
- Click "Advanced" â†’ "Proceed to site"
- This is normal with self-signed certificates
- For production, configure proper SSL certificates

ISSUE: Helm installation fails
SOLUTION:
- Use direct binary installation method
- Check internet connectivity
- Try alternative installation methods provided

14.2 USEFUL TROUBLESHOOTING COMMANDS
# Check pod logs:
kubectl logs <pod-name> -n <namespace>

# Describe resources:
kubectl describe pod <pod-name> -n <namespace>
kubectl describe node <node-name>

# Check events:
kubectl get events --sort-by=.metadata.creationTimestamp

# Restart services:
sudo systemctl restart kubelet
sudo systemctl restart docker
sudo systemctl restart containerd

14.3 RESET CLUSTER (if needed)
# On all nodes (use with caution):
sudo kubeadm reset
sudo rm -rf /etc/kubernetes/
sudo rm -rf ~/.kube/
sudo rm -rf /var/lib/etcd/
sudo systemctl restart kubelet

===========================================
SECTION 15: MAINTENANCE AND BEST PRACTICES
===========================================

15.1 REGULAR MAINTENANCE
- Keep Kubernetes components updated
- Update Rancher regularly
- Monitor cluster health
- Regular backups of etcd
- Monitor resource usage

15.2 BACKUP PROCEDURES
# Backup etcd (Important!):
sudo cp -r /var/lib/etcd /var/lib/etcd-backup-$(date +%Y%m%d-%H%M%S)

# Backup Rancher data:
kubectl get secret --namespace cattle-system -o yaml > rancher-backup.yaml

15.3 SECURITY RECOMMENDATIONS
- Change default Rancher password after first login
- Create additional users with appropriate roles
- Use Role-Based Access Control (RBAC)
- Configure network policies
- Use proper SSL certificates for production
- Regular security updates

15.4 MONITORING SETUP
- Deploy Prometheus and Grafana via Rancher
- Set up alerting rules
- Monitor node and pod resources
- Track cluster events

===========================================
SECTION 16: USEFUL COMMANDS REFERENCE
===========================================

16.1 KUBECTL COMMANDS
# Cluster information:
kubectl cluster-info
kubectl get nodes
kubectl get pods -A
kubectl get svc -A
kubectl get ingress -A

# Pod management:
kubectl logs <pod-name> -n <namespace>
kubectl describe pod <pod-name> -n <namespace>
kubectl exec -it <pod-name> -n <namespace> -- /bin/bash

# Service management:
kubectl get svc -A
kubectl describe svc <service-name> -n <namespace>

16.2 HELM COMMANDS
# Repository management:
helm repo list
helm repo add <name> <url>
helm repo update

# Release management:
helm list -A
helm install <name> <chart>
helm upgrade <name> <chart>
helm uninstall <name>

16.3 RANCHER SPECIFIC COMMANDS
# Get Rancher status:
kubectl get pods -n cattle-system
kubectl get svc -n cattle-system
kubectl logs -n cattle-system -l app=rancher

# Get bootstrap password:
kubectl get secret --namespace cattle-system bootstrap-secret -o go-template='{{.data.bootstrapPassword|base64decode}}{{ "\n" }}'

===========================================
SECTION 17: NEXT STEPS AND ADVANCED FEATURES
===========================================

17.1 EXPLORE RANCHER FEATURES
1. Cluster management and monitoring
2. Application deployment from catalog
3. User and permission management
4. Backup and disaster recovery
5. Multi-cluster management
6. CI/CD pipeline integration

17.2 DEPLOY SAMPLE APPLICATIONS
1. Use Rancher's App Catalog
2. Deploy WordPress, MySQL, etc.
3. Create custom deployments
4. Set up ingress rules
5. Configure persistent volumes

17.3 ADVANCED CONFIGURATIONS
1. Set up monitoring with Prometheus/Grafana
2. Configure log aggregation
3. Implement network policies
4. Set up service mesh (Istio)
5. Configure autoscaling

===========================================
SECTION 18: SUCCESS VERIFICATION CHECKLIST
===========================================

INFRASTRUCTURE SETUP:
â–¡ All three servers updated and configured
â–¡ Docker installed on all servers
â–¡ Kubernetes components installed
â–¡ Hostnames and /etc/hosts configured
â–¡ Firewall rules configured

CLUSTER SETUP:
â–¡ Master node initialized successfully
â–¡ Worker nodes joined successfully
â–¡ CNI network plugin (Calico) installed
â–¡ All nodes showing "Ready" status
â–¡ System pods running properly

RANCHER SETUP:
â–¡ Helm installed
â–¡ Cert-manager deployed and running
â–¡ NGINX Ingress Controller installed
â–¡ Rancher installed and accessible
â–¡ NodePort access working
â–¡ Ingress access working (optional)

VERIFICATION:
â–¡ Test deployment successful
â–¡ All services accessible
â–¡ Rancher UI accessible and functional
â–¡ Cluster visible in Rancher
â–¡ All nodes visible in Rancher

===========================================
FINAL ACCESS SUMMARY
===========================================

YOUR KUBERNETES CLUSTER IS NOW READY!

RANCHER ACCESS:
- âœ… CONFIRMED WORKING: https://10.10.80.76:30507 (Master Node)
- Primary URL: https://10.10.80.76:NODEPORT (check with: kubectl get svc rancher-nodeport -n cattle-system)
- Alternative: https://rancher.local.cluster:30507
- Alternative: https://10.10.80.76:30507

RANCHER CREDENTIALS:
- Username: admin
- Password: admin123456

MONITORING SERVICES ACCESS:
- âœ… Grafana: http://10.10.80.76:31000 (admin/admin123)
- âœ… Prometheus: http://10.10.80.76:31001
- âœ… AlertManager: http://10.10.80.76:31002

MONITORING FEATURES:
- Complete Kubernetes cluster monitoring
- Pre-configured Grafana dashboards
- Prometheus metrics collection
- Alert management with AlertManager
- Node and pod-level metrics
- Resource usage tracking

âœ… DEPLOYMENT STATUS: SUCCESSFUL - Rancher is running on Master Node and accessible!

CLUSTER NODES:
- Master: k8s-master (10.10.80.76)
- Worker 1: k8s-node1 (10.10.80.77)
- Worker 2: k8s-node2 (10.10.80.78)

IMPORTANT SERVICES:
- Kubernetes API: https://10.10.80.76:6443
- Rancher Management: Available via URLs above
- Ingress Controller: Ports 31680 (HTTP), 32085 (HTTPS)

===========================================
SUPPORT AND RESOURCES
===========================================

DOCUMENTATION:
- Kubernetes: https://kubernetes.io/docs/
- Rancher: https://rancher.com/docs/
- Helm: https://helm.sh/docs/

COMMUNITY SUPPORT:
- Kubernetes GitHub: https://github.com/kubernetes/kubernetes
- Rancher GitHub: https://github.com/rancher/rancher
- Stack Overflow: kubernetes, rancher tags

TROUBLESHOOTING:
- Check logs with kubectl logs commands
- Use kubectl describe for detailed information
- Monitor events with kubectl get events
- Check Rancher forums for specific issues

===========================================
ðŸŽ‰ CONGRATULATIONS! DEPLOYMENT SUCCESSFUL! ðŸŽ‰
===========================================

Your Kubernetes cluster with Rancher is now fully operational!
âœ… CONFIRMED WORKING: Rancher accessible at https://10.10.80.76:30507 (Master Node)

You have successfully set up:
âœ“ A 3-node Kubernetes cluster
âœ“ Rancher management interface - RUNNING ON MASTER NODE AND ACCESSIBLE
âœ“ Complete networking and ingress
âœ“ SSL certificates and security
âœ“ Multiple access methods
âœ“ VERIFIED WORKING ACCESS: https://10.10.80.76:30507

You can now:
- Deploy applications through Rancher UI
- Manage your cluster through a web interface
- Scale your applications across multiple nodes
- Monitor and maintain your infrastructure
- Explore advanced Kubernetes features

Happy containerizing with Kubernetes and Rancher!

===========================================
END OF GUIDE
===========================================