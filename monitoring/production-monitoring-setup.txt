# PRODUCTION-READY PROMETHEUS AND GRAFANA SETUP
# For Kubernetes Cluster - Worker Nodes Deployment
# Date: November 4, 2025
# PRODUCTION BEST PRACTICES INCLUDED

===========================================
PRODUCTION DEPLOYMENT STRATEGY
===========================================

PRODUCTION BEST PRACTICES:
‚úÖ Deploy monitoring on WORKER NODES ONLY (not master)
‚úÖ Use node selectors and taints/tolerations
‚úÖ Implement resource limits and requests
‚úÖ Use persistent storage with proper storage classes
‚úÖ Configure high availability (HA) setup
‚úÖ Separate monitoring from control plane
‚úÖ Implement proper backup strategies
‚úÖ Use dedicated monitoring namespace with RBAC

DEPLOYMENT TARGETS:
- Worker Node 1: k8s-node1 (10.10.80.77)
- Worker Node 2: k8s-node2 (10.10.80.78)
- Master Node: k8s-master (10.10.80.76) - CONTROL PLANE ONLY

===========================================
SECTION 1: PRODUCTION PREREQUISITES
===========================================

1.1 LABEL WORKER NODES FOR MONITORING
Run on Master (10.10.80.76):

# Label worker nodes for monitoring workloads
kubectl label node k8s-node1 node-role.kubernetes.io/worker=true
kubectl label node k8s-node2 node-role.kubernetes.io/worker=true
kubectl label node k8s-node1 monitoring=enabled
kubectl label node k8s-node2 monitoring=enabled

# Verify labels
kubectl get nodes --show-labels

1.2 TAINT MASTER NODE (Production Best Practice)
Run on Master (10.10.80.76):

# Taint master to prevent workload scheduling (if not already done)
kubectl taint nodes k8s-master node-role.kubernetes.io/master=true:NoSchedule --overwrite
kubectl taint nodes k8s-master node-role.kubernetes.io/control-plane=true:NoSchedule --overwrite

# Verify taints
kubectl describe node k8s-master | grep -i taint

1.3 CREATE MONITORING NAMESPACE WITH RBAC
kubectl create namespace monitoring

# Apply RBAC and network policies (created in next steps)

===========================================
SECTION 2: PRODUCTION MONITORING INSTALLATION
===========================================

2.1 ADD PROMETHEUS COMMUNITY HELM REPOSITORY
Run on Master (10.10.80.76):

helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update

2.2 INSTALL PRODUCTION MONITORING STACK
Run on Master (10.10.80.76):

# Install with production-ready configuration
helm install prometheus-stack prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --values /home/kp/oraganization-project/k8-cluster/monitoring/production-values.yaml \
  --timeout 15m \
  --wait

2.3 VERIFY PRODUCTION DEPLOYMENT
# Check that pods are NOT running on master node
kubectl get pods -n monitoring -o wide

# Verify all pods are on worker nodes only
kubectl get pods -n monitoring -o wide | grep -v "k8s-master"

# Check resource usage
kubectl top pods -n monitoring
kubectl top nodes

===========================================
SECTION 3: PRODUCTION ACCESS CONFIGURATION
===========================================

3.1 INGRESS-BASED ACCESS (RECOMMENDED FOR PRODUCTION)
# Apply production ingress configuration
kubectl apply -f /home/kp/oraganization-project/k8-cluster/monitoring/production-ingress.yaml

3.2 LOAD BALANCER ACCESS (CLOUD ENVIRONMENTS)
# For cloud deployments, use LoadBalancer services
kubectl patch svc prometheus-stack-grafana -n monitoring -p '{"spec":{"type":"LoadBalancer"}}'
kubectl patch svc prometheus-stack-kube-prom-prometheus -n monitoring -p '{"spec":{"type":"LoadBalancer"}}'

3.3 SECURE ACCESS SETUP
# Create TLS certificates for production
kubectl create secret tls monitoring-tls -n monitoring \
  --cert=/path/to/monitoring.crt \
  --key=/path/to/monitoring.key

===========================================
SECTION 4: HIGH AVAILABILITY CONFIGURATION
===========================================

4.1 PROMETHEUS HA SETUP
# Prometheus is configured with:
# - 2 replicas across worker nodes
# - Anti-affinity rules
# - Persistent storage
# - Resource limits

4.2 GRAFANA HA SETUP
# Grafana is configured with:
# - 2 replicas for high availability
# - Shared storage for dashboards
# - Session affinity
# - Resource limits

4.3 VERIFY HA DEPLOYMENT
kubectl get pods -n monitoring -o wide
kubectl get pvc -n monitoring
kubectl describe deployment prometheus-stack-grafana -n monitoring

===========================================
SECTION 5: PRODUCTION MONITORING ACCESS
===========================================

5.1 SECURE ACCESS URLS

PRODUCTION ACCESS (via Ingress):
- Grafana: https://grafana.k8s.local
- Prometheus: https://prometheus.k8s.local
- AlertManager: https://alertmanager.k8s.local

DEVELOPMENT ACCESS (NodePort - for testing):
- Grafana: http://10.10.80.77:31000 or http://10.10.80.78:31000
- Prometheus: http://10.10.80.77:31001 or http://10.10.80.78:31001
- AlertManager: http://10.10.80.77:31002 or http://10.10.80.78:31002

5.2 PRODUCTION CREDENTIALS
- Grafana Username: admin
- Grafana Password: Use strong password from production-values.yaml
- Prometheus: Authentication via ingress/auth-proxy
- AlertManager: Authentication via ingress/auth-proxy

===========================================
SECTION 6: PRODUCTION BACKUP AND MAINTENANCE
===========================================

6.1 AUTOMATED BACKUP SETUP
# Backup Grafana dashboards and configuration
kubectl create cronjob grafana-backup -n monitoring \
  --image=postgres:13 \
  --schedule="0 2 * * *" \
  --restart=OnFailure \
  -- /bin/sh -c "kubectl exec -n monitoring deployment/prometheus-stack-grafana -- sqlite3 /var/lib/grafana/grafana.db '.backup /tmp/backup.db' && kubectl cp monitoring/prometheus-stack-grafana-xxx:/tmp/backup.db ./grafana-backup-$(date +%Y%m%d).db"

6.2 PROMETHEUS DATA BACKUP
# Prometheus data is automatically persisted to PVCs
# Backup PVCs using your storage provider's backup solution
kubectl get pvc -n monitoring

6.3 MONITORING STACK UPDATES
# Update monitoring stack
helm repo update
helm upgrade prometheus-stack prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --values /home/kp/oraganization-project/k8-cluster/monitoring/production-values.yaml

===========================================
SECTION 7: PRODUCTION SECURITY HARDENING
===========================================

7.1 NETWORK POLICIES
# Apply network policies to restrict traffic
kubectl apply -f /home/kp/oraganization-project/k8-cluster/monitoring/network-policies.yaml

7.2 RBAC CONFIGURATION
# Monitoring services run with minimal required permissions
# Custom service accounts with restricted access

7.3 SECRET MANAGEMENT
# Use Kubernetes secrets for sensitive configuration
# Rotate credentials regularly
# Use external secret management (Vault, etc.) for production

===========================================
SECTION 8: PRODUCTION PERFORMANCE TUNING
===========================================

8.1 RESOURCE ALLOCATION
RECOMMENDED RESOURCES PER WORKER NODE:
- CPU: 4+ cores per worker node
- Memory: 8GB+ per worker node
- Storage: 100GB+ for monitoring data

8.2 PROMETHEUS CONFIGURATION
- Retention: 30 days (configurable)
- Scrape intervals optimized for production
- Query performance optimization
- Compaction settings

8.3 GRAFANA OPTIMIZATION
- Dashboard caching
- Query optimization
- Plugin management
- Session management

===========================================
SECTION 9: PRODUCTION MONITORING BEST PRACTICES
===========================================

9.1 WORKLOAD SEPARATION
‚úÖ Control plane on master node only
‚úÖ Monitoring workloads on worker nodes only
‚úÖ Application workloads on dedicated nodes (if available)
‚úÖ Proper resource allocation and limits

9.2 SCALABILITY CONSIDERATIONS
‚úÖ Horizontal Pod Autoscaler (HPA) for Grafana
‚úÖ Vertical Pod Autoscaler (VPA) for optimal resource usage
‚úÖ Cluster autoscaling for additional worker nodes
‚úÖ Monitoring data retention policies

9.3 OPERATIONAL EXCELLENCE
‚úÖ Automated alerting for critical issues
‚úÖ Regular backup verification
‚úÖ Performance monitoring and optimization
‚úÖ Security scanning and updates
‚úÖ Documentation and runbooks

===========================================
PRODUCTION DEPLOYMENT VERIFICATION
===========================================

PRODUCTION CHECKLIST:
‚ñ° Monitoring pods deployed on worker nodes ONLY
‚ñ° Master node free of monitoring workloads
‚ñ° High availability configured (2+ replicas)
‚ñ° Persistent storage configured and working
‚ñ° Resource limits and requests set appropriately
‚ñ° Network policies applied for security
‚ñ° RBAC configured with minimal permissions
‚ñ° TLS/SSL certificates configured
‚ñ° Backup procedures in place
‚ñ° Monitoring and alerting operational

PERFORMANCE VERIFICATION:
‚ñ° All nodes have adequate resources
‚ñ° No resource contention on master node
‚ñ° Monitoring stack responding within SLA
‚ñ° Data retention working as configured
‚ñ° Alerts firing appropriately
‚ñ° Dashboards loading quickly

SECURITY VERIFICATION:
‚ñ° No unnecessary services exposed
‚ñ° Strong authentication configured
‚ñ° Network traffic restricted appropriately
‚ñ° Regular security updates applied
‚ñ° Secrets properly managed
‚ñ° Audit logging enabled

===========================================
PRODUCTION ACCESS SUMMARY
===========================================

üè≠ PRODUCTION DEPLOYMENT SUCCESSFUL!

ARCHITECTURE:
‚úÖ Master Node (10.10.80.76): Control plane ONLY
‚úÖ Worker Node 1 (10.10.80.77): Monitoring + Applications
‚úÖ Worker Node 2 (10.10.80.78): Monitoring + Applications

MONITORING ACCESS:
‚úÖ Grafana: https://grafana.k8s.local (Production)
‚úÖ Prometheus: https://prometheus.k8s.local (Production)
‚úÖ AlertManager: https://alertmanager.k8s.local (Production)

PRODUCTION FEATURES:
‚úì High availability across worker nodes
‚úì Persistent storage with backup
‚úì Resource limits and monitoring
‚úì Security hardening applied
‚úì Network policies enforced
‚úì Automated scaling configured
‚úì Production-grade access controls

Your monitoring stack is now production-ready and follows industry best practices!

===========================================
END OF PRODUCTION MONITORING GUIDE
===========================================