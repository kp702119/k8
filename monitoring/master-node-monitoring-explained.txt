# MASTER NODE MONITORING EXPLAINED
# Why and How We Monitor Kubernetes Master Nodes
# Date: November 4, 2025

===========================================
MASTER NODE MONITORING STRATEGY
===========================================

üéØ PRODUCTION PRINCIPLE:
"Monitor everything, but deploy workloads strategically"

‚úÖ WE DO MONITOR THE MASTER NODE
‚ùå WE DON'T DEPLOY HEAVY WORKLOADS ON MASTER

===========================================
WHAT RUNS ON MASTER NODE
===========================================

KUBERNETES CONTROL PLANE (Always):
‚úÖ kube-apiserver (API Server)
‚úÖ etcd (Cluster database)
‚úÖ kube-scheduler (Pod scheduling)
‚úÖ kube-controller-manager (Controllers)
‚úÖ kubelet (Node management)
‚úÖ kube-proxy (Network proxy)

MONITORING COMPONENTS (Lightweight):
‚úÖ Node Exporter (system metrics)
‚úÖ cAdvisor (container metrics)
‚úÖ Prometheus scrapers (metric collection endpoints)

WHAT WE DON'T PUT ON MASTER:
‚ùå Grafana server (dashboard application)
‚ùå Prometheus server (metric storage & processing)
‚ùå AlertManager server (alert processing)
‚ùå Application workloads
‚ùå Database workloads

===========================================
MASTER NODE METRICS WE COLLECT
===========================================

1. SYSTEM METRICS (via Node Exporter):
   - CPU usage, memory, disk I/O
   - Network interfaces and traffic
   - Filesystem usage and inodes
   - System load and processes
   - Hardware sensors (temperature, etc.)

2. KUBERNETES API SERVER METRICS:
   - API request rates and latency
   - Authentication and authorization
   - Admission controller performance
   - Resource usage per API endpoint

3. ETCD METRICS (Critical!):
   - Database size and growth
   - Read/write operations
   - Raft consensus health
   - Backup and snapshot status

4. SCHEDULER METRICS:
   - Pod scheduling latency
   - Scheduling failures and retries
   - Resource constraints and availability

5. CONTROLLER MANAGER METRICS:
   - Controller loop performance
   - Resource reconciliation
   - Leader election status

6. KUBELET METRICS:
   - Pod lifecycle management
   - Container runtime performance
   - Volume mount operations
   - Image pull performance

===========================================
WHY THIS APPROACH IS PRODUCTION BEST PRACTICE
===========================================

üèóÔ∏è SEPARATION OF CONCERNS:
- Master node focuses on cluster management
- Worker nodes handle application workloads
- Monitoring infrastructure scales independently

üöÄ PERFORMANCE & RELIABILITY:
- Master node resources dedicated to control plane
- No resource competition from monitoring workloads
- Faster API server response times
- More stable etcd performance

üõ°Ô∏è SECURITY & ISOLATION:
- Reduced attack surface on master node
- Control plane isolation from applications
- Easier security auditing and compliance

üìà SCALABILITY:
- Monitoring can scale horizontally on worker nodes
- Master node performance predictable
- Easy to add more worker nodes for monitoring capacity

===========================================
MONITORING VERIFICATION COMMANDS
===========================================

1. CHECK NODE EXPORTER ON ALL NODES:
kubectl get pods -n monitoring -o wide | grep node-exporter

# Should show node-exporter on ALL nodes including master:
# prometheus-node-exporter-abc123   k8s-master
# prometheus-node-exporter-def456   k8s-node1  
# prometheus-node-exporter-ghi789   k8s-node2

2. VERIFY CONTROL PLANE METRICS:
kubectl get servicemonitors -n monitoring

# Should show ServiceMonitors for:
# - kube-apiserver
# - kube-controller-manager
# - kube-etcd
# - kube-scheduler
# - kubelet

3. CHECK PROMETHEUS TARGETS:
# Port-forward to Prometheus and check targets
kubectl port-forward -n monitoring svc/prometheus-stack-kube-prom-prometheus 9090:9090

# Then visit http://localhost:9090/targets
# Look for master node targets:
# - node-exporter (k8s-master:9100)
# - apiserver (k8s-master:6443)
# - kubelet (k8s-master:10250)
# - etcd (k8s-master:2379)

4. VERIFY MASTER NODE METRICS IN GRAFANA:
# Login to Grafana and check dashboards:
# - "Kubernetes / Compute Resources / Cluster"
# - "Kubernetes / API server"
# - "etcd"
# Master node should appear in all relevant dashboards

===========================================
GRAFANA DASHBOARDS FOR MASTER MONITORING
===========================================

CONTROL PLANE DASHBOARDS:
1. "Kubernetes / API server" - API server performance
2. "etcd" - etcd cluster health and performance  
3. "Kubernetes / Scheduler" - Scheduler performance
4. "Kubernetes / Controller Manager" - Controller health

SYSTEM DASHBOARDS:
1. "Node Exporter / Nodes" - System metrics for all nodes
2. "Kubernetes / Compute Resources / Cluster" - Cluster overview
3. "Kubernetes / Networking / Cluster" - Network performance

CUSTOM QUERIES FOR MASTER MONITORING:
# CPU usage on master node:
rate(node_cpu_seconds_total{instance="k8s-master:9100"}[5m])

# API server request rate:
rate(apiserver_request_total[5m])

# etcd database size:
etcd_mvcc_db_total_size_in_bytes

# Scheduler latency:
histogram_quantile(0.99, scheduler_scheduling_duration_seconds_bucket)

===========================================
PRODUCTION MONITORING CHECKLIST
===========================================

MASTER NODE MONITORING:
‚ñ° Node Exporter running on master node
‚ñ° API server metrics being collected
‚ñ° etcd metrics being monitored  
‚ñ° Scheduler performance tracked
‚ñ° Controller manager health monitored
‚ñ° Kubelet metrics collected
‚ñ° System resources (CPU/memory/disk) monitored

WORKLOAD SEPARATION:
‚ñ° NO Grafana pods on master node
‚ñ° NO Prometheus server pods on master node
‚ñ° NO AlertManager pods on master node
‚ñ° NO application workloads on master node
‚ñ° All heavy workloads on worker nodes only

VERIFICATION:
‚ñ° Master node visible in Grafana dashboards
‚ñ° Control plane metrics available in Prometheus
‚ñ° Alerts configured for master node issues
‚ñ° Master node performance within acceptable limits

===========================================
EXAMPLE: MASTER NODE IN GRAFANA
===========================================

When you open Grafana dashboards, you SHOULD see:

1. NODE OVERVIEW DASHBOARD:
   - k8s-master (10.10.80.76) - CPU: 15%, Memory: 2GB, Load: 0.8
   - k8s-node1 (10.10.80.77) - CPU: 45%, Memory: 6GB, Load: 2.1
   - k8s-node2 (10.10.80.78) - CPU: 38%, Memory: 5GB, Load: 1.9

2. KUBERNETES CLUSTER DASHBOARD:
   - API Server: Healthy, 500 req/sec
   - etcd: Healthy, 2.1GB database size
   - Scheduler: 150ms avg latency
   - All nodes: 3/3 ready

3. CONTROL PLANE HEALTH:
   - Master node system metrics
   - Control plane component metrics
   - Cluster-wide resource usage

===========================================
TROUBLESHOOTING MASTER NODE MONITORING
===========================================

ISSUE: Master node not visible in Grafana
SOLUTION:
1. Check if Node Exporter is running on master:
   kubectl get pods -n monitoring -o wide | grep node-exporter
2. Verify tolerations allow pods on master
3. Check if master node is properly labeled

ISSUE: API server metrics missing
SOLUTION:
1. Verify API server ServiceMonitor:
   kubectl get servicemonitor -n monitoring
2. Check if API server exposes metrics endpoint
3. Verify network connectivity from Prometheus to master

ISSUE: etcd metrics not available
SOLUTION:
1. Check if etcd metrics are enabled
2. Verify etcd ServiceMonitor configuration
3. Check etcd endpoint accessibility

===========================================
SUMMARY
===========================================

‚úÖ YES, WE MONITOR THE MASTER NODE COMPREHENSIVELY
‚úÖ We collect system metrics (Node Exporter)
‚úÖ We monitor all control plane components
‚úÖ We track API server, etcd, scheduler performance
‚úÖ Master node appears in all relevant Grafana dashboards

‚ùå NO, WE DON'T PUT HEAVY WORKLOADS ON MASTER
‚ùå Grafana server runs on worker nodes
‚ùå Prometheus server runs on worker nodes  
‚ùå AlertManager runs on worker nodes
‚ùå Application workloads stay off master

This approach gives you:
üéØ Complete visibility into master node health
üöÄ Optimal performance for control plane
üõ°Ô∏è Production-grade reliability and security
üìä Comprehensive monitoring without compromising stability

Your master node IS being monitored - just in the right way for production!

===========================================
END OF MASTER NODE MONITORING GUIDE
===========================================