# Production Values for Prometheus Stack Helm Chart
# File: production-values.yaml
# Deploys monitoring on WORKER NODES ONLY

# Global settings for production
global:
  imageRegistry: ""
  imagePullSecrets: []

# Common labels and selectors
commonLabels:
  environment: "production"
  cluster: "k8s-cluster"

# Default rules configuration
defaultRules:
  create: true
  rules:
    alertmanager: true
    etcd: true
    configReloaders: true
    general: true
    k8s: true
    kubeApiserverAvailability: true
    kubeApiserverBurnrate: true
    kubeApiserverHistogram: true
    kubeApiserverSlos: true
    kubelet: true
    kubeProxy: true
    kubePrometheusGeneral: true
    kubePrometheusNodeRecording: true
    kubernetesApps: true
    kubernetesResources: true
    kubernetesStorage: true
    kubernetesSystem: true
    node: true
    nodeExporterAlerting: true
    nodeExporterRecording: true
    prometheus: true
    prometheusOperator: true

# AlertManager - Production Configuration
alertmanager:
  enabled: true
  serviceAccount:
    create: true
    name: ""
    annotations: {}
  
  # Production service configuration
  service:
    type: ClusterIP  # Use ClusterIP for ingress-based access
    port: 9093
    targetPort: 9093
    annotations: {}
    labels: {}
    clusterIP: ""
    
  # Production AlertManager specification
  alertmanagerSpec:
    # Deploy on worker nodes only
    nodeSelector:
      node-role.kubernetes.io/worker: "true"
    tolerations: []
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - alertmanager
            topologyKey: kubernetes.io/hostname
    
    # Production resource requirements
    resources:
      requests:
        memory: "200Mi"
        cpu: "100m"
      limits:
        memory: "500Mi"
        cpu: "200m"
    
    # High availability
    replicas: 2
    
    # Storage configuration
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: ""  # Use default storage class
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi
    
    # Data retention
    retention: 120h
    
    # Security context
    securityContext:
      runAsNonRoot: true
      runAsUser: 1000
      fsGroup: 2000

# Grafana - Production Configuration
grafana:
  enabled: true
  
  # Production admin credentials (change in production!)
  adminUser: admin
  adminPassword: "ProD@Grafan@2025!"  # Change this in production!
  
  # Service configuration for ingress
  service:
    type: ClusterIP
    port: 80
    targetPort: 3000
    annotations: {}
    labels: {}
  
  # Deploy on worker nodes only
  nodeSelector:
    node-role.kubernetes.io/worker: "true"
  tolerations: []
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/name
              operator: In
              values:
              - grafana
          topologyKey: kubernetes.io/hostname
  
  # High availability
  replicas: 2
  
  # Production resource requirements
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "512Mi"
      cpu: "200m"
  
  # Persistent storage
  persistence:
    enabled: true
    type: pvc
    storageClassName: ""  # Use default storage class
    accessModes:
      - ReadWriteOnce
    size: 20Gi
    annotations: {}
    finalizers:
      - kubernetes.io/pvc-protection
  
  # Security configuration
  securityContext:
    runAsNonRoot: true
    runAsUser: 472
    runAsGroup: 472
    fsGroup: 472
  
  # Grafana configuration
  grafana.ini:
    server:
      protocol: http
      http_port: 3000
      domain: grafana.k8s.local
      root_url: https://grafana.k8s.local
      serve_from_sub_path: false
    security:
      admin_user: admin
      admin_password: "ProD@Grafan@2025!"
      secret_key: "SW2YcwTIb9zpOOhoPsMm"
    auth:
      disable_login_form: false
    auth.anonymous:
      enabled: false
    analytics:
      reporting_enabled: false
      check_for_updates: false
    log:
      mode: console
      level: info
  
  # Dashboard providers
  sidecar:
    dashboards:
      enabled: true
      searchNamespace: monitoring
      provider:
        foldersFromFilesStructure: true
    datasources:
      enabled: true
      searchNamespace: monitoring

# Prometheus - Production Configuration
prometheus:
  enabled: true
  
  serviceAccount:
    create: true
    name: ""
    annotations: {}
  
  # Service configuration for ingress
  service:
    type: ClusterIP
    port: 9090
    targetPort: 9090
    annotations: {}
    labels: {}
  
  # Production Prometheus specification
  prometheusSpec:
    # Deploy on worker nodes only
    nodeSelector:
      node-role.kubernetes.io/worker: "true"
    tolerations: []
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - prometheus
            topologyKey: kubernetes.io/hostname
    
    # High availability
    replicas: 2
    
    # Production resource requirements
    resources:
      requests:
        memory: "2Gi"
        cpu: "500m"
      limits:
        memory: "4Gi"
        cpu: "1"
    
    # Storage configuration
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: ""  # Use default storage class
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi
    
    # Data retention and performance
    retention: 30d
    retentionSize: 45GB
    walCompression: true
    
    # Service discovery
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    ruleSelectorNilUsesHelmValues: false
    
    # Security context
    securityContext:
      runAsNonRoot: true
      runAsUser: 1000
      fsGroup: 2000
    
    # Additional scrape configs for production monitoring
    additionalScrapeConfigs: []

# Prometheus Operator Configuration
prometheusOperator:
  enabled: true
  
  # Deploy on worker nodes only
  nodeSelector:
    node-role.kubernetes.io/worker: "true"
  tolerations: []
  affinity: {}
  
  # Production resource requirements
  resources:
    requests:
      memory: "100Mi"
      cpu: "50m"
    limits:
      memory: "200Mi"
      cpu: "100m"
  
  # Security context
  securityContext:
    runAsNonRoot: true
    runAsUser: 65534

# Node Exporter Configuration
nodeExporter:
  enabled: true
  
  # Deploy on ALL nodes including master (for system metrics monitoring)
  tolerations:
    - operator: Exists
      effect: NoSchedule
    - operator: Exists
      effect: NoExecute
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
  
  # Resources
  resources:
    requests:
      memory: "50Mi"
      cpu: "20m"
    limits:
      memory: "100Mi"
      cpu: "50m"

# Kube State Metrics Configuration
kubeStateMetrics:
  enabled: true
  
  # Deploy on worker nodes only
  nodeSelector:
    node-role.kubernetes.io/worker: "true"
  tolerations: []
  
  # Resources
  resources:
    requests:
      memory: "100Mi"
      cpu: "50m"
    limits:
      memory: "200Mi"
      cpu: "100m"

# Prometheus Node Exporter
prometheus-node-exporter:
  enabled: true
  
  # Deploy on ALL nodes including master for complete system monitoring
  tolerations:
    - operator: Exists
      effect: NoSchedule
    - operator: Exists
      effect: NoExecute
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule
  
  # Enable master node monitoring
  hostNetwork: true
  hostPID: true
  
  resources:
    requests:
      memory: "50Mi"
      cpu: "20m"
    limits:
      memory: "100Mi"
      cpu: "50m"

# Control Plane Monitoring (Master Node Components)
coreDns:
  enabled: true

kubeDns:
  enabled: false

# etcd monitoring (critical for cluster state)
kubeEtcd:
  enabled: true
  service:
    enabled: true
    port: 2379
    targetPort: 2379

# Kubernetes Scheduler monitoring
kubeScheduler:
  enabled: true
  service:
    enabled: true
    port: 10259
    targetPort: 10259

# Kube Proxy monitoring (on all nodes)
kubeProxy:
  enabled: true
  service:
    enabled: true
    port: 10249
    targetPort: 10249

# Controller Manager monitoring
kubeControllerManager:
  enabled: true
  service:
    enabled: true
    port: 10257
    targetPort: 10257

# API Server monitoring (most critical)
kubeApiServer:
  enabled: true
  serviceMonitor:
    enabled: true
    additionalLabels: {}

# Kubelet monitoring (on all nodes including master)
kubelet:
  enabled: true
  serviceMonitor:
    enabled: true
    # Monitor master node kubelet too
    additionalLabels: {}